#%% Imports
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_score, recall_score
from lazypredict.Supervised import LazyClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from utils import print_evaluation

#%% Load preprocessed embeddings
print("="*70)
print("LOADING EMBEDDINGS")
print("="*70)

DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(DIR, 'data')

# Load embeddings generated by embed.py
print("\nLoading pre-generated embeddings...")
X_train_embed = np.load(os.path.join(DB_PATH, 'train_embeddings.npy'))
X_test_embed = np.load(os.path.join(DB_PATH, 'test_embeddings.npy'))
y_train = np.load(os.path.join(DB_PATH, 'y_train.npy'))
y_test = np.load(os.path.join(DB_PATH, 'y_test.npy'))

print(f"✓ Loaded {len(X_train_embed)} training samples")
print(f"✓ Loaded {len(X_test_embed)} test samples")
print(f"✓ Embedding dimensions: {X_train_embed.shape[1]}")
print(f"\nTraining set shape: {X_train_embed.shape}")
print(f"Test set shape: {X_test_embed.shape}")

#%% Data verification
print("\n" + "="*70)
print("DATA VERIFICATION")
print("="*70)

print(f"\nTraining set distribution:")
print(f"  Ham (0): {sum(y_train == 0)} ({sum(y_train == 0)/len(y_train)*100:.1f}%)")
print(f"  Spam (1): {sum(y_train == 1)} ({sum(y_train == 1)/len(y_train)*100:.1f}%)")

print(f"\nTest set distribution:")
print(f"  Ham (0): {sum(y_test == 0)} ({sum(y_test == 0)/len(y_test)*100:.1f}%)")
print(f"  Spam (1): {sum(y_test == 1)} ({sum(y_test == 1)/len(y_test)*100:.1f}%)")

# Check for NaN or inf values
print(f"\nData quality checks:")
print(f"  Training NaN values: {np.isnan(X_train_embed).sum()}")
print(f"  Training Inf values: {np.isinf(X_train_embed).sum()}")
print(f"  Test NaN values: {np.isnan(X_test_embed).sum()}")
print(f"  Test Inf values: {np.isinf(X_test_embed).sum()}")

#%% LazyPredict - Quick Model Comparison with Embeddings
print("\n" + "="*70)
print("LAZYPREDICT - EMBEDDING-BASED MODEL COMPARISON")
print("="*70)

print("\nConverting embeddings to DataFrames for LazyPredict...")
X_train_df = pd.DataFrame(X_train_embed)
X_test_df = pd.DataFrame(X_test_embed)
print(f"✓ Converted to DataFrames: {X_train_df.shape}, {X_test_df.shape}")

print("\nTesting 30+ classification models with transformer embeddings...")
print("(This may take a few minutes...)\n")

clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train_df, X_test_df, y_train, y_test)

# Calculate Precision and Recall for each model
print("\nCalculating Precision and Recall for all models...")
precision_scores = []
recall_scores = []

for model_name in models.index:
    try:
        # Get predictions for this model
        y_pred = predictions[model_name].values
        precision = precision_score(y_test, y_pred, average='binary', zero_division=0)
        recall = recall_score(y_test, y_pred, average='binary', zero_division=0)
        precision_scores.append(precision)
        recall_scores.append(recall)
    except KeyError:
        # Model failed and has no predictions
        precision_scores.append(np.nan)
        recall_scores.append(np.nan)
    except Exception as e:
        # Other errors
        print(f"⚠ Error processing {model_name}: {e}")
        precision_scores.append(np.nan)
        recall_scores.append(np.nan)

# Add Precision and Recall columns to models DataFrame
models['Precision'] = precision_scores
models['Recall'] = recall_scores

print(f"✓ Evaluated {len(models)} models with Accuracy, F1-Score, ROC AUC, Precision, and Recall")

# Display results sorted by F1-Score
print("\nTop 15 Models by F1-Score:")
print("="*70)
top_models = models.sort_values('F1 Score', ascending=False).head(15)
print(top_models.to_string())

#%% Visualize Embedding-based Results
print("\n" + "="*70)
print("MODEL PERFORMANCE VISUALIZATION")
print("="*70)

# Create graphs directory if it doesn't exist
graphs_dir = 'graphs'
if not os.path.exists(graphs_dir):
    os.makedirs(graphs_dir)
    print(f"✓ Created {graphs_dir}/ directory")

# Plot top 10 models by different metrics
fig, axes = plt.subplots(2, 3, figsize=(20, 12))
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']
top_n = 10

for idx, metric in enumerate(metrics):
    ax = axes[idx // 3, idx % 3]

    # Get top 10 models by this metric
    top_data = models.sort_values(metric, ascending=False).head(top_n)

    # Create horizontal bar plot
    y_pos = np.arange(len(top_data))
    ax.barh(y_pos, top_data[metric], color='forestgreen', alpha=0.8)
    ax.set_yticks(y_pos)
    ax.set_yticklabels(top_data.index, fontsize=9)
    ax.set_xlabel(metric, fontsize=10)
    ax.set_title(f'Top {top_n} Models by {metric} (Embeddings)', fontsize=12, fontweight='bold')

    # Dynamically adjust x-axis for better visibility
    min_val = top_data[metric].min()
    max_val = top_data[metric].max()

    if min_val > 0.9:
        # Zoom in to show differences in high-performing models
        x_min = max(0, min_val - 0.03)
        x_max = min(1.0, max_val + 0.01)
        ax.set_xlim([x_min, x_max])
    elif min_val > 0.85:
        # Moderate zoom for good performers
        x_min = max(0, min_val - 0.05)
        x_max = min(1.0, max_val + 0.02)
        ax.set_xlim([x_min, x_max])
    else:
        # Use standard 0-1 range for more varied scores
        ax.set_xlim([0, 1])

    ax.grid(axis='x', alpha=0.3)
    ax.invert_yaxis()

# Hide the empty subplot
axes[1, 2].axis('off')

plt.tight_layout()
plt.savefig(os.path.join(graphs_dir, 'embedding_ml_comparison.png'), dpi=300, bbox_inches='tight')
print(f"✓ Saved visualization to {graphs_dir}/embedding_ml_comparison.png")
plt.close()

#%% Identify Top Performers
print("\n" + "="*70)
print("TOP PERFORMERS ANALYSIS")
print("="*70)

# Get top 3 models by F1-score
top_3 = models.sort_values('F1 Score', ascending=False).head(3)

print("\nTop 3 Models for Fine-Tuning:")
for i, (model_name, scores) in enumerate(top_3.iterrows(), 1):
    print(f"\n{i}. {model_name}")
    print(f"   Accuracy: {scores['Accuracy']:.4f}")
    print(f"   Precision: {scores['Precision']:.4f}")
    print(f"   Recall: {scores['Recall']:.4f}")
    print(f"   F1-Score: {scores['F1 Score']:.4f}")
    print(f"   ROC AUC: {scores['ROC AUC']:.4f}")

# Save results to CSV
models.to_csv(os.path.join(graphs_dir, 'embedding_ml_results.csv'))
print(f"\n✓ Full results saved to {graphs_dir}/embedding_ml_results.csv")

#%% Detailed Evaluation of Best Model
print("\n" + "="*70)
print("BEST MODEL DETAILED EVALUATION")
print("="*70)

best_model_name = models.sort_values('F1 Score', ascending=False).index[0]
print(f"\nBest Model: {best_model_name}")

# Get predictions from LazyPredict
if best_model_name in predictions.columns:
    best_predictions = predictions[best_model_name]
    # Print detailed evaluation
    print_evaluation(y_test, best_predictions, title=f"{best_model_name} Performance (Embeddings)")
else:
    print(f"⚠ Cannot evaluate {best_model_name} - predictions not available")

#%% Compare with Traditional ML
print("\n" + "="*70)
print("COMPARISON WITH TRADITIONAL ML")
print("="*70)

# Try to load traditional ML results for comparison
try:
    traditional_results = pd.read_csv(os.path.join(graphs_dir, 'traditional_ml_results.csv'), index_col=0)

    # Get best models from each approach
    best_traditional = traditional_results.sort_values('F1 Score', ascending=False).iloc[0]
    best_embedding = models.sort_values('F1 Score', ascending=False).iloc[0]

    print("\nBest Traditional ML Model:")
    print(f"  Model: {best_traditional.name}")
    print(f"  F1-Score: {best_traditional['F1 Score']:.4f}")
    print(f"  Accuracy: {best_traditional['Accuracy']:.4f}")
    print(f"  Precision: {best_traditional['Precision']:.4f}")
    print(f"  Recall: {best_traditional['Recall']:.4f}")

    print("\nBest Embedding-based Model:")
    print(f"  Model: {best_embedding.name}")
    print(f"  F1-Score: {best_embedding['F1 Score']:.4f}")
    print(f"  Accuracy: {best_embedding['Accuracy']:.4f}")
    print(f"  Precision: {best_embedding['Precision']:.4f}")
    print(f"  Recall: {best_embedding['Recall']:.4f}")

    # Calculate improvement
    f1_improvement = ((best_embedding['F1 Score'] - best_traditional['F1 Score'])
                      / best_traditional['F1 Score'] * 100)

    print(f"\nF1-Score Improvement: {f1_improvement:+.2f}%")

    if f1_improvement > 0:
        print("✓ Embedding-based approach outperforms traditional ML")
    elif f1_improvement < 0:
        print("⚠ Traditional ML outperforms embedding-based approach")
    else:
        print("≈ Both approaches perform similarly")

except FileNotFoundError:
    print("\n⚠ Traditional ML results not found. Run traditional_ml.py first for comparison.")

#%% Save model artifacts
print("\n" + "="*70)
print("SAVING ARTIFACTS")
print("="*70)

# Save embedding-based results summary
summary = {
    'best_model': models.sort_values('F1 Score', ascending=False).index[0],
    'best_f1': models.sort_values('F1 Score', ascending=False).iloc[0]['F1 Score'],
    'best_accuracy': models.sort_values('F1 Score', ascending=False).iloc[0]['Accuracy'],
    'embedding_model': 'all-MiniLM-L6-v2',
    'embedding_dim': X_train_embed.shape[1],
    'n_train_samples': len(X_train_embed),
    'n_test_samples': len(X_test_embed)
}

import json
with open(os.path.join(graphs_dir, 'embedding_summary.json'), 'w') as f:
    json.dump(summary, f, indent=2)

print(f"✓ Saved summary to {graphs_dir}/embedding_summary.json")

print("\n" + "="*70)
print("EMBEDDING-BASED CLASSIFICATION COMPLETE")
print("="*70)
print(f"\nResults:")
print(f"  - Best Model: {summary['best_model']}")
print(f"  - F1-Score: {summary['best_f1']:.4f}")
print(f"  - Accuracy: {summary['best_accuracy']:.4f}")
print(f"\nNext steps:")
print("  1. Fine-tune top models with hyperparameter optimization")
print("  2. Experiment with different embedding models")
print("  3. Try ensemble methods combining both approaches")

# %%
